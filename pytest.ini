[tool:pytest]
# Pytest configuration file

# Test discovery
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Test directories
testpaths = tests

# Minimum version
minversion = 6.0

# Add options
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --durations=10
    --color=yes
    --disable-warnings
    --ignore=setup.py
    --ignore=build/
    --ignore=dist/
    --ignore=.eggs/
    --ignore=venv/
    --ignore=env/
    --ignore=.venv/
    --ignore=.env/

# Markers
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    gpu: marks tests that require GPU
    external: marks tests that require external services
    unit: marks tests as unit tests
    smoke: marks tests as smoke tests
    regression: marks tests as regression tests
    performance: marks tests as performance tests
    security: marks tests as security tests

# Logging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Warnings
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::FutureWarning
    ignore:.*CUDA.*:UserWarning
    ignore:.*torch.*:UserWarning
    ignore:.*transformers.*:UserWarning

# Test timeout (in seconds)
timeout = 300

# Parallel execution
# Uncomment to enable parallel test execution
# addopts = -n auto

# Coverage settings (when using pytest-cov)
# addopts = --cov=src --cov-report=html --cov-report=term-missing --cov-fail-under=80

# JUnit XML output (for CI/CD)
# addopts = --junitxml=test-results.xml

# HTML report (when using pytest-html)
# addopts = --html=test-report.html --self-contained-html

# Performance testing (when using pytest-benchmark)
# addopts = --benchmark-skip

# Test order randomization (when using pytest-randomly)
# addopts = --randomly-seed=1234

# Doctest settings
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL ELLIPSIS

# Test collection
collect_ignore = [
    "setup.py",
    "conftest.py",
    "build",
    "dist",
    ".eggs",
    "venv",
    "env",
    ".venv",
    ".env",
    "node_modules",
    ".git",
    "__pycache__"
]

# Test execution
# norecursedirs = .git .tox dist build *.egg venv env .venv .env

# Test output
console_output_style = progress

# Test discovery patterns
python_files = test_*.py *_test.py tests.py
python_classes = Test* *Tests
python_functions = test_*

# Test session
# cache_dir = .pytest_cache

# Test reporting
# resultlog = test-results.log

# Test fixtures
# usefixtures = 

# Test plugins
# required_plugins = pytest-cov>=2.0 pytest-html>=2.0

# Test environment
# env = 
#     TESTING = true
#     PYTHONPATH = src

# Test markers for different environments
# markers =
#     unit: Unit tests
#     integration: Integration tests
#     e2e: End-to-end tests
#     smoke: Smoke tests
#     regression: Regression tests
#     performance: Performance tests
#     security: Security tests
#     slow: Slow tests
#     fast: Fast tests
#     gpu: Tests requiring GPU
#     cpu: Tests requiring only CPU
#     external: Tests requiring external services
#     offline: Tests that can run offline
#     online: Tests requiring internet connection